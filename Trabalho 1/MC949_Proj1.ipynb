{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AisSk1wNpr"
      },
      "source": [
        "# Preparar Ambiente de excecução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65mx8FvgfMlO",
        "outputId": "465aebb6-9dfd-44e9-f1d5-f14e491d7e9b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import least_squares\n",
        "\n",
        "print(f\"Usando OpenCV versão: {cv2.__version__}\")\n",
        "\n",
        "try:\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    from google.colab import files\n",
        "    colab = True\n",
        "except ImportError:\n",
        "    colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definição de caminhos e variáveis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = './output/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "cenario = 'cenario3'\n",
        "n_imagens = 7\n",
        "\n",
        "input_dir = f'./fotos/{cenario}/'\n",
        "\n",
        "output_dir = f'./output/{cenario}/'\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Carregar imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if colab:\n",
        "    upload = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJSE-qQugA_G",
        "outputId": "ebdfb3ee-3506-43e5-a8a8-e58adb793fc5"
      },
      "outputs": [],
      "source": [
        "img_names = [x for x in os.listdir(input_dir) if x.endswith(('.jpeg', '.jpg', '.png'))]\n",
        "\n",
        "# --- LÓGICA DE ORDENAÇÃO NUMÉRICA ---\n",
        "def extrair_numero(nome_do_arquivo):\n",
        "    try:\n",
        "        parte_sem_extensao = nome_do_arquivo.split('.')[0]\n",
        "        numero_str = parte_sem_extensao.split('_')[1]\n",
        "        return int(numero_str)\n",
        "    except:\n",
        "        return -1\n",
        "\n",
        "# Usa a função como chave para a ordenação (1, 2, 3...)\n",
        "img_names.sort(key=extrair_numero)\n",
        "# --- FIM DA LÓGICA DE ORDENAÇÃO ---\n",
        "\n",
        "\n",
        "# >>> ADICIONE ESTA LINHA PARA INVERTER A ORDEM <<<\n",
        "# Isso transforma a ordem [direita...esquerda] em [esquerda...direita]\n",
        "img_names.reverse()\n",
        "\n",
        "\n",
        "# Imprime a lista final para você conferir\n",
        "print(\"Ordem de arquivos corrigida (esquerda para a direita):\")\n",
        "print(img_names)\n",
        "\n",
        "# O resto do código continua igual, carregando as imagens na ordem correta\n",
        "images = [cv2.imread(input_dir + nome) for nome in img_names][:n_imagens]  # Carrega todas as imagens na ordem correta\n",
        "images_cinza = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images]\n",
        "\n",
        "print(f\"\\nCarregadas {len(images)} imagens na ordem correta para a montagem.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teste para comparação com cv2.Stitcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a Stitcher object\n",
        "stitcher = cv2.Stitcher_create()\n",
        "\n",
        "# Stitch the images\n",
        "status, stitched_image = stitcher.stitch(images)\n",
        "\n",
        "# Check the stitching status\n",
        "if status == cv2.Stitcher_OK:\n",
        "    plt.figure(figsize=(20, 12))\n",
        "    plt.imshow(cv2.cvtColor(stitched_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    # Salvar a imagem \n",
        "    output_final = output_dir + 'final/'\n",
        "    os.makedirs(output_final, exist_ok=True)\n",
        "    cv2.imwrite(output_final + f'panorama_{cenario}_stitched.jpg', stitched_image)\n",
        "\n",
        "else:\n",
        "    print(f\"Image stitching failed with status: {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMNilP8gwnux"
      },
      "source": [
        "# Detecção e Extração de Características"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o3c9nschZUP"
      },
      "source": [
        "Utilizando o SIFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fmg7uVAghZL_",
        "outputId": "6038ae3b-922d-41f1-9657-4130cfdd3f19"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "SIFT parameters\n",
        "nfeatures:\tLimits the max number of strongest keypoints returned. If too high, too many points; lower it to reduce keypoints.\n",
        "contrastThreshold:\tHigher values reduce keypoints in low-contrast areas. Default 0.04. Increase to detect fewer points.\n",
        "edgeThreshold\tControls: edge response filtering. Higher values allow more edge points; lower removes edge-like keypoints.\n",
        "nOctaveLayers:\tMore layers = finer scale-space detection; can increase number of keypoints slightly.\n",
        "sigma:\tInitial Gaussian blur. Larger sigma smooths image, reducing noise and small keypoints.\n",
        "\"\"\"\n",
        "sift = cv2.SIFT_create(\n",
        "    nfeatures=1000,          # Maximum number of keypoints to retain\n",
        "    nOctaveLayers=3,         # Number of layers in each octave (affects scale space granularity)\n",
        "    contrastThreshold=0.04,  # Minimum contrast for keypoints\n",
        "    edgeThreshold=10,        # Minimum edge response (reduces edge keypoints)\n",
        "    sigma=1.2                # Gaussian smoothing applied at first octave\n",
        ")\n",
        "\n",
        "# Contains sift (keypoints, desc)\n",
        "# sift = lista de tuplas:\n",
        "# sift[0][0] = keypoint da imagem 0\n",
        "# sift[0][1] = descriptor da imagem 0\n",
        "# sift[1][0] = keypoint da imagem 1 (...)\n",
        "sift = [sift.detectAndCompute(im, None) for im in images_cinza]\n",
        "\n",
        "\"\"\"\n",
        "cv2.drawKeypoints(image, keypoints, outImage, color=None, flags=0) draws keypoints on an image.\n",
        "Parameters:\n",
        "image: Original BGR image.\n",
        "keypoints: List of detected keypoints.\n",
        "outImage: Output image. None means OpenCV creates a copy.\n",
        "flags: How keypoints are drawn. Key options:\n",
        "  cv2.DRAW_MATCHES_FLAGS_DEFAULT: Simple small circle.\n",
        "  cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS: Draws size and orientation of keypoints (circles with lines).\n",
        "  cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS: Ignores points with low response.\n",
        "\"\"\"\n",
        "img_sift = cv2.drawKeypoints(images[0], sift[0][0], None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.imshow(cv2.cvtColor(img_sift, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "img_sift = cv2.drawKeypoints(images[1], sift[1][0], None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.imshow(cv2.cvtColor(img_sift, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd88KRDxivpk"
      },
      "source": [
        "Utilizando o ORB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "collapsed": true,
        "id": "E7My7mMshWUF",
        "outputId": "08b13de3-9b85-4fa4-f7d2-ffe9135998d8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ORB parameters\n",
        "nfeatures:\tMaximum number of keypoints returned. Higher = more points.\n",
        "scaleFactor:\tScale pyramid factor. Each level is prev_level / scaleFactor. Smaller factor → more levels → more keypoints.\n",
        "nlevels:\tNumber of pyramid levels. More levels → detects features at more scales.\n",
        "edgeThreshold:\tMargin around image where keypoints are ignored. Prevents keypoints too close to edges.\n",
        "firstLevel:\tBase level of pyramid. Usually 0.\n",
        "WTA_K:\tNumber of points used for computing BRIEF descriptor (2,3,4). Higher = more distinctive but slower.\n",
        "scoreType:\tHow keypoints are ranked:\n",
        "  cv2.ORB_HARRIS_SCORE → uses Harris corner score.\n",
        "  cv2.ORB_FAST_SCORE → uses FAST score. |\n",
        "  | patchSize | Size of patch around keypoint for descriptor computation. Larger = more context, slower. |\n",
        "  | fastThreshold | Threshold for FAST corner detection. Higher → fewer keypoints; lower → more keypoints. |\n",
        "\"\"\"\n",
        "orb = cv2.ORB_create(\n",
        "    nfeatures=200,         # Max number of keypoints to retain\n",
        "    scaleFactor=1.2,       # Pyramid decimation ratio between levels\n",
        "    nlevels=4,             # Number of pyramid levels\n",
        "    edgeThreshold=31,      # Size of border to ignore edges\n",
        "    firstLevel=0,          # Index of first pyramid level\n",
        "    WTA_K=2,               # Number of points for BRIEF descriptor (2,3,4)\n",
        "    scoreType=cv2.ORB_HARRIS_SCORE,  # Method to rank keypoints\n",
        "    patchSize=31,          # Size of patch used by BRIEF descriptor\n",
        "    fastThreshold=20       # Threshold for FAST keypoint detection\n",
        ")\n",
        "\n",
        "# Contains orb (keypoints, desc)\n",
        "orb = [orb.detectAndCompute(im, None) for im in images_cinza]\n",
        "\n",
        "img_orb = cv2.drawKeypoints(images[0], orb[0][0], None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.imshow(cv2.cvtColor(img_orb, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ordenação Automática"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contagem_inliers(kp1, des1, kp2, des2, ratio=0.75, ransac_thresh=4.0):\n",
        "    if des1 is None or des2 is None or len(des1) < 4 or len(des2) < 4:\n",
        "        return 0\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
        "    raw = bf.knnMatch(des1, des2, k=2)\n",
        "    boas = [m for m, n in raw if m.distance < ratio * n.distance]\n",
        "    if len(boas) < 4:\n",
        "        return 0\n",
        "    pts1 = np.float32([kp1[m.queryIdx].pt for m in boas])\n",
        "    pts2 = np.float32([kp2[m.trainIdx].pt for m in boas])\n",
        "    H, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC, ransac_thresh)\n",
        "    if mask is None:\n",
        "        return 0\n",
        "    return int(mask.sum())  # nº de inliers\n",
        "\n",
        "# 2) Matriz de inliers entre todos os pares\n",
        "def matriz_inliers(sifts):\n",
        "    n = len(sifts)\n",
        "    W = np.zeros((n, n), dtype=int)\n",
        "    for i in range(n):\n",
        "        kpi, desi = sifts[i]\n",
        "        for j in range(i+1, n):\n",
        "            kpj, desj = sifts[j]\n",
        "            w = contagem_inliers(kpi, desi, kpj, desj)\n",
        "            W[i, j] = W[j, i] = w\n",
        "    return W\n",
        "\n",
        "# 3) Constrói um grafo “quase-caminho”: para cada nó, guarda só os 1–2 melhores vizinhos acima de um limiar\n",
        "def construir_adjacencias(W, top_k=2, min_inliers=None):\n",
        "    n = W.shape[0]\n",
        "    if min_inliers is None:\n",
        "        # limiar adaptativo: pelo menos 15 inliers, ou 15% do melhor par\n",
        "        min_inliers = max(15, int(0.15 * W.max()))\n",
        "    adj = [set() for _ in range(n)]\n",
        "    for i in range(n):\n",
        "        ordem = np.argsort(-W[i])  # decrescente\n",
        "        candidatos = [j for j in ordem if j != i and W[i, j] >= min_inliers][:top_k]\n",
        "        for j in candidatos:\n",
        "            adj[i].add(j)\n",
        "            adj[j].add(i)\n",
        "    # mantém apenas arestas mútuas (consistência)\n",
        "    adj = [sorted([ (j, W[i, j]) for j in a if i in adj[j] ], key=lambda x: -x[1]) for i, a in enumerate(adj)]\n",
        "    return adj, min_inliers\n",
        "\n",
        "# 4) Caminha a partir de uma folha; se for um 360° sem folhas, começa no mais “conectado”\n",
        "def ordenar_panorama_linear(W):\n",
        "    \"\"\"\n",
        "    Encontra a ordem das imagens para um panorama linear.\n",
        "    Assume que W[i, j] contém o número de inliers entre a imagem i e j.\n",
        "    \"\"\"\n",
        "    n = W.shape[0]\n",
        "    if n == 0:\n",
        "        return []\n",
        "\n",
        "    # --- CORREÇÃO AQUI ---\n",
        "    # Convertemos a matriz para float para que ela possa aceitar o valor 'np.inf'.\n",
        "    # A função .astype(float) cria uma nova cópia já com o tipo correto.\n",
        "    W_temp = W.astype(float)\n",
        "\n",
        "    # O resto do código permanece igual\n",
        "    # Para encontrar as pontas, procuramos o par com a menor contagem de inliers (mas que não seja zero).\n",
        "    W_temp[W_temp == 0] = np.inf\n",
        "    np.fill_diagonal(W_temp, np.inf)\n",
        "\n",
        "    # Encontra o índice do menor valor na matriz achatada\n",
        "    min_idx_flat = np.argmin(W_temp)\n",
        "    # Converte o índice achatado de volta para coordenadas 2D (linha, coluna)\n",
        "    ponta1, ponta2 = np.unravel_index(min_idx_flat, W_temp.shape)\n",
        "\n",
        "    # Agora, começamos a ordenação a partir de uma das pontas\n",
        "    ordem = [ponta1]\n",
        "    visitados = {ponta1}\n",
        "\n",
        "    cur = ponta1\n",
        "\n",
        "    while len(ordem) < n:\n",
        "        # Encontra o melhor vizinho para a imagem atual 'cur'\n",
        "        melhor_vizinho = -1\n",
        "        max_inliers = -1\n",
        "\n",
        "        # Itera sobre todas as possíveis imagens vizinhas\n",
        "        for vizinho in range(n):\n",
        "            # O vizinho não pode ter sido visitado ainda\n",
        "            if vizinho not in visitados:\n",
        "                # Usamos a matriz W original para comparar os inliers\n",
        "                if W[cur, vizinho] > max_inliers:\n",
        "                    max_inliers = W[cur, vizinho]\n",
        "                    melhor_vizinho = vizinho\n",
        "\n",
        "        # Se um vizinho válido foi encontrado\n",
        "        if melhor_vizinho != -1:\n",
        "            ordem.append(melhor_vizinho)\n",
        "            visitados.add(melhor_vizinho)\n",
        "            cur = melhor_vizinho\n",
        "        else:\n",
        "            # Se chegarmos a um beco sem saída (não deve acontecer em um panorama bem conectado)\n",
        "            restantes = [i for i in range(n) if i not in visitados]\n",
        "            ordem.extend(restantes)\n",
        "            break\n",
        "\n",
        "    return ordem\n",
        "\n",
        "\n",
        "# ====== USO ======\n",
        "W = matriz_inliers(sift)\n",
        "adj, th = construir_adjacencias(W, top_k=2)  # 2 vizinhos é o esperado num panorama\n",
        "ordem = ordenar_panorama_linear(W)\n",
        "ordem = list(reversed(ordem))\n",
        "\n",
        "print(\"Matriz de inliers (W) – maiores valores indicam sobreposição forte:\\n\", W)\n",
        "print(\"Ordem encontrada (índices):\", ordem)\n",
        "print(\"Arquivos na ordem:\")\n",
        "for i in ordem:\n",
        "    print(img_names[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smRkSRqfl9UM"
      },
      "source": [
        "# Emparelhamento de Características"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "hOnenlit0iVT",
        "outputId": "b12c2564-8050-4580-e605-ee8a9faf67cd"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Brute-Force Matcer\n",
        "\n",
        "Brute-Force matcher is simple. It takes the descriptor of one feature in first set and is matched with all other features in second set using some distance calculation.\n",
        "And the closest one is returned.\n",
        "\n",
        "Lets take as an example images 1 and 2\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# BFMatcher with default params\n",
        "bf = cv2.BFMatcher()\n",
        "# Corrected indices to access the first two images (index 0 and 1)\n",
        "matches = bf.knnMatch(sift[0][1], sift[1][1], k=2)\n",
        "\n",
        "# Apply ratio test\n",
        "good = []\n",
        "for m,n in matches:\n",
        "    if m.distance < 0.75 * n.distance:\n",
        "        good.append([m])\n",
        "\n",
        "# cv2.drawMatchesKnn expects list of lists as matches.\n",
        "# Corrected indices for images and sift to match the first two images\n",
        "matched_image = cv2.drawMatchesKnn(images[0], sift[0][0], images[1], sift[1][0], good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "plt.imshow(matched_image),plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXD3c9_Ti3s1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "FLANN = Fast Library for Approximate Nearest Neighbors, optimized for large datasets and high-dimensional descriptors.\n",
        "\n",
        "For FLANN based matcher, we need to pass two dictionaries which specifies the algorithm to be used, its related parameters etc.\n",
        "\n",
        "Parameters:\n",
        "  algorithm=1 → KD-tree (best for SIFT/SURF descriptors).\n",
        "  trees=5 → number of KD-trees in the index (more trees = higher accuracy, slower).\n",
        "  checks=50 → number of times to check trees for nearest neighbors (higher = more accurate, but takes more time).\n",
        "\"\"\"\n",
        "FLANN_INDEX_KDTREE = 1\n",
        "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "search_params = dict(checks=50)\n",
        "\n",
        "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "\"\"\"\n",
        "knnMatch() finds the two nearest neighbors for each descriptor.\n",
        "We use k=2 because Lowe’s ratio test requires two closest matches.\n",
        "\n",
        "A descriptor is a numerical vector that describes the local image patch around a keypoint.\n",
        "For SIFT, each descriptor is 128-dimensional.\n",
        "Descriptors encode scale, orientation, and gradient information, so that we can compare keypoints across images.\n",
        "\n",
        "matches = flann.knnMatch(des1, des2, k=2)\n",
        "des1 → descriptors from image 1\n",
        "des2 → descriptors from image 2\n",
        "flann.knnMatch() compares all vectors in des1 to all vectors in des2 and finds the closest matches using Euclidean distance.\n",
        "\"\"\"\n",
        "matches = []\n",
        "for i in range(len(sift) - 1):\n",
        "  mtchs = flann.knnMatch(sift[i][1], sift[i + 1][1], k = 2) # Total matches\n",
        "  # Lowe's tet ration\n",
        "  \"\"\"\n",
        "  m = best match, n = second-best match.\n",
        "  m.distance = Euclidean distance between descriptors.\n",
        "  Ratio test keeps matches where best match is significantly closer than second-best.\n",
        "  Threshold 0.7 is typical (adjustable).\n",
        "  \"\"\"\n",
        "  good_matches = []\n",
        "  for m, n in mtchs:\n",
        "      if m.distance < 0.6 * n.distance:\n",
        "          good_matches.append(m)\n",
        "\n",
        "  print(f\"{i} and {i+1} Good matches after ratio test: {len(good_matches)}\")\n",
        "  matches.append(good_matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "LJcDC5h4mrfT",
        "outputId": "ec779f9b-854b-405a-c4d1-278d71de4112"
      },
      "outputs": [],
      "source": [
        "img_matches = cv2.drawMatches(\n",
        "    images_cinza[2], sift[2][0],\n",
        "    images_cinza[3], sift[3][0],\n",
        "    matches[2], None,\n",
        "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        ")\n",
        "\"\"\"\n",
        "cv2.drawMatches() draws lines connecting matching keypoints.\n",
        "flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS → only draws matched keypoints, not all keypoints.\n",
        "\"\"\"\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49sph1Q3wtvv"
      },
      "source": [
        "# Estimação de Homografia e Alinhamento (Falta generalizar para todas a imagens e arrumar a propagação de erro das deformações)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funções da homografia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsaPArL69U0z"
      },
      "outputs": [],
      "source": [
        "def encontrar_correspondencias(desc1, desc2):\n",
        "    \"\"\"Encontra correspondências entre dois conjuntos de descritores usando o teste de razão de Lowe.\"\"\"\n",
        "    bf = cv2.BFMatcher()\n",
        "    # Encontra as 2 melhores correspondências para cada descritor\n",
        "    matches = bf.knnMatch(desc1, desc2, k=2)\n",
        "\n",
        "    # Aplica o teste de razão para manter apenas as boas correspondências\n",
        "    boas_correspondencias = []\n",
        "    for m, n in matches:\n",
        "        if m.distance < 0.75 * n.distance:\n",
        "            boas_correspondencias.append(m)\n",
        "    return boas_correspondencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkNEOXHZRiID"
      },
      "outputs": [],
      "source": [
        "def encontrar_homografia(correspondencias, kp1, kp2):\n",
        "    \"\"\"Calcula a matriz de homografia usando RANSAC a partir das correspondências.\"\"\"\n",
        "    # Requer pelo menos 4 correspondências para calcular a homografia\n",
        "    if len(correspondencias) > 4:\n",
        "        # Extrai as localizações dos pontos correspondentes\n",
        "        pontos_origem = np.float32([kp1[m.queryIdx].pt for m in correspondencias]).reshape(-1, 1, 2)\n",
        "        pontos_destino = np.float32([kp2[m.trainIdx].pt for m in correspondencias]).reshape(-1, 1, 2)\n",
        "\n",
        "        # Encontra a homografia usando o algoritmo RANSAC\n",
        "        # O 5.0 é o RANSAC reprojection threshold: distância máxima em pixels\n",
        "        # para um ponto ser considerado inlier.\n",
        "        matriz_homografia, mascara = cv2.findHomography(pontos_origem, pontos_destino, cv2.RANSAC, 5.0)\n",
        "\n",
        "        return matriz_homografia, mascara\n",
        "    return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ilZhGtLpuuw"
      },
      "outputs": [],
      "source": [
        "def calcular_todas_as_homografias(images, sifts):\n",
        "    \"\"\"Calcula as homografias entre todos os pares de imagens adjacentes.\"\"\"\n",
        "    homografias = []\n",
        "    for i in range(len(images) - 1):\n",
        "        kp1, desc1 = sifts[i]\n",
        "        kp2, desc2 = sifts[i+1]\n",
        "\n",
        "        # Usa as funções que você já tem\n",
        "        correspondencias = encontrar_correspondencias(desc1, desc2)\n",
        "        H, _ = encontrar_homografia(correspondencias, kp1, kp2)\n",
        "\n",
        "        if H is None:\n",
        "            raise ValueError(f\"Não foi possível calcular a homografia entre a imagem {i} e {i+1}\")\n",
        "\n",
        "        homografias.append(H)\n",
        "\n",
        "    return homografias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0Xni1wgqoF1"
      },
      "outputs": [],
      "source": [
        "def calcular_homografias_para_referencia(homografias_parciais, imagem_referencia_idx):\n",
        "    \"\"\"Calcula as homografias de todas as imagens em relação a uma imagem de referência.\"\"\"\n",
        "    num_imagens = len(homografias_parciais) + 1\n",
        "    homografias_globais = [None] * num_imagens\n",
        "\n",
        "    # A homografia da imagem de referência para ela mesma é a matriz identidade\n",
        "    H_ref_atual = np.identity(3)\n",
        "    homografias_globais[imagem_referencia_idx] = H_ref_atual\n",
        "\n",
        "    # Calcula as homografias para as imagens à DIREITA da referência\n",
        "    for i in range(imagem_referencia_idx, num_imagens - 1):\n",
        "        H_inv = np.linalg.inv(homografias_parciais[i])\n",
        "        H_ref_atual = H_ref_atual.dot(H_inv)\n",
        "        homografias_globais[i+1] = H_ref_atual\n",
        "\n",
        "    # Calcula as homografias para as imagens à ESQUERDA da referência\n",
        "    H_ref_atual = np.identity(3)\n",
        "    for i in range(imagem_referencia_idx - 1, -1, -1):\n",
        "        H_ref_atual = H_ref_atual.dot(homografias_parciais[i])\n",
        "        homografias_globais[i] = H_ref_atual\n",
        "\n",
        "    return homografias_globais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4Q4B0Uy3ps"
      },
      "source": [
        "# Composição e Blending"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geração do panorama final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Função de renderização do panorama com blending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2woc7olYRp1D"
      },
      "outputs": [],
      "source": [
        "def renderizar_panorama_final(images, homografias):\n",
        "    \"\"\"\n",
        "    Renderiza o panorama final deformando todas as imagens para um canvas comum.\n",
        "    \"\"\"\n",
        "    # Encontra os limites do panorama final\n",
        "    x_min, y_min = np.inf, np.inf\n",
        "    x_max, y_max = -np.inf, -np.inf\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        h, w = img.shape[:2]\n",
        "        cantos = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
        "\n",
        "        # Projeta os cantos da imagem no plano do panorama\n",
        "        H = homografias[i]\n",
        "        cantos_transformados = cv2.perspectiveTransform(cantos, H)\n",
        "\n",
        "        # Atualiza os mínimos e máximos\n",
        "        x_min = min(x_min, np.min(cantos_transformados[:,:,0]))\n",
        "        x_max = max(x_max, np.max(cantos_transformados[:,:,0]))\n",
        "        y_min = min(y_min, np.min(cantos_transformados[:,:,1]))\n",
        "        y_max = max(y_max, np.max(cantos_transformados[:,:,1]))\n",
        "\n",
        "    # Cria uma matriz de translação para garantir que todas as coordenadas sejam positivas\n",
        "    matriz_translacao = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]])\n",
        "\n",
        "    largura_final = int(np.ceil(x_max - x_min))\n",
        "    altura_final = int(np.ceil(y_max - y_min))\n",
        "\n",
        "    # Prepara o canvas final\n",
        "    panorama = np.zeros((altura_final, largura_final, 3), dtype=np.float32)\n",
        "\n",
        "    pesos = np.zeros((altura_final, largura_final), dtype=np.float32)\n",
        "\n",
        "    # Processa cada imagem\n",
        "    for i, img in enumerate(images):\n",
        "\n",
        "        H_final = matriz_translacao.dot(homografias[i])\n",
        "\n",
        "        img_deformada = cv2.warpPerspective(img, H_final, (largura_final, altura_final))\n",
        "\n",
        "        # Máscara binária\n",
        "        mascara = np.any(img_deformada > 0, axis=2).astype(np.float32)\n",
        "\n",
        "        # Distância até a borda (gera feather suave)\n",
        "        dist = cv2.distanceTransform(mascara.astype(np.uint8), cv2.DIST_L2, 5)\n",
        "        dist = dist / (dist.max() + 1e-5)  # normalização 0-1\n",
        "\n",
        "        # Atualiza blending\n",
        "        for c in range(3):\n",
        "            panorama[:,:,c] += img_deformada[:,:,c] * dist\n",
        "        pesos += dist\n",
        "\n",
        "    # Evita divisão por zero\n",
        "    pesos[pesos == 0] = 1\n",
        "    panorama /= pesos[:,:,None]\n",
        "\n",
        "    return panorama.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mostra a evolução da montagem do panorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualizar_evolucao_panorama(images, sifts_data_list):\n",
        "    \"\"\"\n",
        "    Gera e exibe o panorama de forma incremental, adicionando uma imagem de cada vez.\n",
        "    \"\"\"\n",
        "    if len(images) < 2:\n",
        "        print(\"São necessárias pelo menos 2 imagens para criar um panorama.\")\n",
        "        return\n",
        "\n",
        "    # Itera de um conjunto de 2 imagens até o conjunto completo\n",
        "    for i in range(2, len(images) + 1):\n",
        "        print(f\"\\n--- Gerando panorama com as {i} primeiras imagens ---\")\n",
        "\n",
        "        imagens_atuais = images[:i]\n",
        "        sifts_atuais = sifts_data_list[:i]\n",
        "\n",
        "        try:\n",
        "            # 1. Calcula as homografias parciais para o subconjunto\n",
        "            homografias_parciais = calcular_todas_as_homografias(imagens_atuais, sifts_atuais)\n",
        "\n",
        "            # 2. Calcula as homografias globais em relação à imagem de referência\n",
        "            idx_referencia = len(imagens_atuais) // 2\n",
        "            homografias_globais = calcular_homografias_para_referencia(homografias_parciais, idx_referencia)\n",
        "\n",
        "            # 3. Renderiza o panorama parcial\n",
        "            panorama_parcial = renderizar_panorama_final(imagens_atuais, homografias_globais)\n",
        "            # panorama_parcial = renderizar_panorama_naive(imagens_atuais, homografias_globais)\n",
        "\n",
        "\n",
        "            # 4. Exibe o resultado intermediário\n",
        "            plt.figure(figsize=(15, 8))\n",
        "            plt.imshow(cv2.cvtColor(panorama_parcial, cv2.COLOR_BGR2RGB))\n",
        "            plt.title(f'Panorama com as {i} primeiras imagens')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Erro ao processar o conjunto de {i} imagens: {e}\")\n",
        "            print(\"Interrompendo a visualização incremental.\")\n",
        "            break\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXECUÇÃO PRINCIPAL\n",
        "# ==============================================================================\n",
        "# Chama a função para mostrar a evolução do panorama\n",
        "visualizar_evolucao_panorama(images, sift)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Montagem do Panorama final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "ZWYwo5IF75z5",
        "outputId": "5d83f123-f9e2-4597-cf4a-ed380bd25ead"
      },
      "outputs": [],
      "source": [
        "print(\"--- Iniciando a criação do panorama utilizando o blending via feathering ---\")\n",
        "\n",
        "# Passo 1: Calcular homografias entre pares de imagens\n",
        "homografias_parciais = calcular_todas_as_homografias(images, sift)\n",
        "\n",
        "# Passo 2: Calcular homografias em relação à imagem central\n",
        "idx_ref_final = len(images) // 2\n",
        "homografias_para_referencia = calcular_homografias_para_referencia(homografias_parciais, idx_ref_final)\n",
        "\n",
        "# Passo 3: Renderizar o panorama\n",
        "panorama_final = renderizar_panorama_final(images, homografias_para_referencia)\n",
        "\n",
        "# Exibir o resultado\n",
        "print(\"--- Panorama concluído! ---\")\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.imshow(cv2.cvtColor(panorama_final, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Panorama com Blending via Feathering\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Salvar a imagem \n",
        "output_final = output_dir + 'final/'\n",
        "os.makedirs(output_final, exist_ok=True)\n",
        "\n",
        "cv2.imwrite(output_final + f'panorama_{cenario}_final.jpg', panorama_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evolução do trabalho passo a passo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diferentes funções de renderização do panorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def renderizar_panorama_naive(images, homografias):\n",
        "    \"\"\"\n",
        "    Renderiza o panorama final deformando todas as imagens para um canvas comum.\n",
        "    \"\"\"\n",
        "    # Encontra os limites do panorama final\n",
        "    x_min, y_min = np.inf, np.inf\n",
        "    x_max, y_max = -np.inf, -np.inf\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        h, w = img.shape[:2]\n",
        "        cantos = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
        "\n",
        "        # Projeta os cantos da imagem no plano do panorama\n",
        "        H = homografias[i]\n",
        "        cantos_transformados = cv2.perspectiveTransform(cantos, H)\n",
        "\n",
        "        # Atualiza os mínimos e máximos\n",
        "        x_min = min(x_min, np.min(cantos_transformados[:,:,0]))\n",
        "        x_max = max(x_max, np.max(cantos_transformados[:,:,0]))\n",
        "        y_min = min(y_min, np.min(cantos_transformados[:,:,1]))\n",
        "        y_max = max(y_max, np.max(cantos_transformados[:,:,1]))\n",
        "\n",
        "    # Cria uma matriz de translação para garantir que todas as coordenadas sejam positivas\n",
        "    matriz_translacao = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]])\n",
        "\n",
        "    largura_final = int(np.ceil(x_max - x_min))\n",
        "    altura_final = int(np.ceil(y_max - y_min))\n",
        "\n",
        "    # Prepara o canvas final\n",
        "    panorama = np.zeros((altura_final, largura_final, 3), dtype=np.uint8)\n",
        "\n",
        "    # Deforma e coloca cada imagem no canvas\n",
        "    for i, img in enumerate(images):\n",
        "            \n",
        "        # Aplica a translação à homografia de cada imagem\n",
        "        H_final = matriz_translacao.dot(homografias[i])\n",
        "\n",
        "        # Deforma a imagem\n",
        "        img_deformada = cv2.warpPerspective(img, H_final, (largura_final, altura_final))\n",
        "        # Cria uma máscara para a área não preta da imagem deformada\n",
        "        mascara_nao_preta = np.all(img_deformada != [0, 0, 0], axis=2)\n",
        "        \n",
        "        # Combina a imagem deformada com o panorama\n",
        "        panorama[mascara_nao_preta] = img_deformada[mascara_nao_preta]\n",
        "\n",
        "    return panorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Renderizar via Operações morfológicas\n",
        "\n",
        "def match_histograms(img1, img2, mask=None):\n",
        "    # converte para LAB (mais robusto)\n",
        "    img1_lab = cv2.cvtColor(img1, cv2.COLOR_BGR2LAB)\n",
        "    img2_lab = cv2.cvtColor(img2, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "    # calcula média e desvio do canal L (luminosidade)\n",
        "    l1, a1, b1 = cv2.split(img1_lab)\n",
        "    l2, a2, b2 = cv2.split(img2_lab)\n",
        "\n",
        "    mean1, std1 = cv2.meanStdDev(l1, mask=mask)\n",
        "    mean2, std2 = cv2.meanStdDev(l2, mask=mask)\n",
        "\n",
        "    # corrige img2 para igualar img1\n",
        "    l2_norm = ((l2 - mean2[0][0]) * (std1[0][0] / (std2[0][0] + 1e-6)) + mean1[0][0])\n",
        "    l2_norm = np.clip(l2_norm, 0, 255).astype(np.uint8)\n",
        "\n",
        "    img2_lab = cv2.merge([l2_norm, a2, b2])\n",
        "    img2_corr = cv2.cvtColor(img2_lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    return img2_corr\n",
        "\n",
        "def renderizar_panorama_morfologia(images, homografias):\n",
        "    \"\"\"\n",
        "    Renderiza o panorama final deformando todas as imagens para um canvas comum.\n",
        "    \"\"\"\n",
        "    # Encontra os limites do panorama final\n",
        "    x_min, y_min = np.inf, np.inf\n",
        "    x_max, y_max = -np.inf, -np.inf\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        h, w = img.shape[:2]\n",
        "        cantos = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
        "\n",
        "        # Projeta os cantos da imagem no plano do panorama\n",
        "        H = homografias[i]\n",
        "        cantos_transformados = cv2.perspectiveTransform(cantos, H)\n",
        "\n",
        "        # Atualiza os mínimos e máximos\n",
        "        x_min = min(x_min, np.min(cantos_transformados[:,:,0]))\n",
        "        x_max = max(x_max, np.max(cantos_transformados[:,:,0]))\n",
        "        y_min = min(y_min, np.min(cantos_transformados[:,:,1]))\n",
        "        y_max = max(y_max, np.max(cantos_transformados[:,:,1]))\n",
        "\n",
        "    # Cria uma matriz de translação para garantir que todas as coordenadas sejam positivas\n",
        "    matriz_translacao = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]])\n",
        "\n",
        "    largura_final = int(np.ceil(x_max - x_min))\n",
        "    altura_final = int(np.ceil(y_max - y_min))\n",
        "\n",
        "    # Prepara o canvas final\n",
        "    panorama = np.zeros((altura_final, largura_final, 3), dtype=np.uint8)\n",
        "\n",
        "    # Deforma e coloca cada imagem no canvas\n",
        "    for i, img in enumerate(images):\n",
        "\n",
        "        if i > 0:\n",
        "            img = match_histograms(images[0], img)\n",
        "            \n",
        "        # Aplica a translação à homografia de cada imagem\n",
        "        H_final = matriz_translacao.dot(homografias[i])\n",
        "\n",
        "        # Deforma a imagem\n",
        "        img_deformada = cv2.warpPerspective(img, H_final, (largura_final, altura_final))\n",
        "        # Cria uma máscara para a área não preta da imagem deformada\n",
        "        mascara_nao_preta = np.all(img_deformada != [0, 0, 0], axis=2)\n",
        "        # Encontra a máscara de sobreposição para evitar sobrescrever com pixels pretos\n",
        "        mascara_nao_preta = cv2.dilate(mascara_nao_preta.astype(np.uint8), np.ones((5, 5), np.uint8)).astype(bool) # Dilatação para preencher buracos\n",
        "        mascara_nao_preta = cv2.erode(mascara_nao_preta.astype(np.uint8), np.ones((9, 9), np.uint8)).astype(bool) # Erosão para remover borda preta\n",
        "        \n",
        "        # Combina a imagem deformada com o panorama\n",
        "        panorama[mascara_nao_preta] = img_deformada[mascara_nao_preta]\n",
        "\n",
        "    return panorama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bordas visíveis e correção por filtro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encontra_bordas(panorama_naive):\n",
        "    teste = panorama_naive.copy()\n",
        "\n",
        "    # panorama_gray = cv2.cvtColor(teste, cv2.COLOR_BGR2GRAY)\n",
        "    panorama_gray_0 = teste[:,:,0]\n",
        "    panorama_gray_1 = teste[:,:,1]\n",
        "    panorama_gray_2 = teste[:,:,2]\n",
        "\n",
        "    # Aplica filtro Laplaciano (passa-alta)\n",
        "    bordas_0 = cv2.Laplacian(panorama_gray_0, cv2.CV_64F)\n",
        "    bordas_0 = np.abs(bordas_0)\n",
        "    bordas_0 = cv2.normalize(bordas_0, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    # Aplica filtro Laplaciano (passa-alta)\n",
        "    bordas_1 = cv2.Laplacian(panorama_gray_1, cv2.CV_64F)\n",
        "    bordas_1 = np.abs(bordas_1)\n",
        "    bordas_1 = cv2.normalize(bordas_1, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    # Aplica filtro Laplaciano (passa-alta)\n",
        "    bordas_2 = cv2.Laplacian(panorama_gray_2, cv2.CV_64F)\n",
        "    bordas_2 = np.abs(bordas_2)\n",
        "    bordas_2 = cv2.normalize(bordas_2, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    bordas = (bordas_0 + bordas_1 + bordas_2) // 3\n",
        "\n",
        "    # Cria uma máscara binária para destacar apenas as linhas mais fortes\n",
        "    _, mascara_bordas = cv2.threshold(bordas, 30, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Visualiza apenas as linhas sobre fundo branco\n",
        "    imagem_linhas = np.ones_like(teste) * 0\n",
        "    imagem_linhas[mascara_bordas > 0] = [255, 255, 255]\n",
        "\n",
        "    imagem_linhas = cv2.dilate(imagem_linhas, np.ones((5, 5), np.uint8))\n",
        "    imagem_linhas = cv2.erode(imagem_linhas, np.ones((3, 3), np.uint8))\n",
        "\n",
        "    return imagem_linhas\n",
        "\n",
        "def aplica_filtro_mascarado(panorama_naive, imagem_linhas):\n",
        "\n",
        "    imagem = panorama_naive.copy()\n",
        "\n",
        "    # 1. Aplica o filtro desejado na imagem inteira\n",
        "    imagem_filtrada = cv2.medianBlur(imagem, 5)\n",
        "\n",
        "    imagem_filtrada = np.abs(imagem_filtrada)\n",
        "    imagem_filtrada = cv2.normalize(imagem_filtrada, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    # 2. Cria uma cópia da imagem original para receber o filtro apenas nas áreas da máscara\n",
        "    imagem_resultado = imagem.copy()\n",
        "\n",
        "    # 3. Aplica o filtro apenas onde a máscara está ativada\n",
        "    imagem_resultado[imagem_linhas > 0] = imagem_filtrada[imagem_linhas > 0]\n",
        "\n",
        "    return imagem_resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"--- Iniciando a criação do panorama naive com correção por bordas ---\")\n",
        "\n",
        "# Passo 1: Calcular homografias entre pares de imagens\n",
        "homografias_parciais_naive = calcular_todas_as_homografias(images, sift)\n",
        "\n",
        "# Passo 2: Calcular homografias em relação à imagem central\n",
        "idx_ref_naive = len(images) // 2\n",
        "homografias_para_referencia = calcular_homografias_para_referencia(homografias_parciais_naive, idx_ref_naive)\n",
        "\n",
        "# Passo 3: Renderizar o panorama\n",
        "panorama_naive = renderizar_panorama_naive(images, homografias_para_referencia)\n",
        "# Passo 4: Encontrar bordas\n",
        "imagem_linhas_1 = encontra_bordas(panorama_naive)\n",
        "# Passo 5: Aplicar filtro mascarado\n",
        "imagem_resultado_1 = aplica_filtro_mascarado(panorama_naive, imagem_linhas_1)\n",
        "\n",
        "\n",
        "# Exibir a imagem antes, as linhas encontradas, e a imagem após o filtro\n",
        "\n",
        "fig_1, axis_1 = plt.subplots(1, 3, figsize=(60, 12))\n",
        "# Exibir a imagem original\n",
        "axis_1[0].imshow(cv2.cvtColor(panorama_naive, cv2.COLOR_BGR2RGB))\n",
        "axis_1[0].set_title(\"Panorama gerado inicialmente\")\n",
        "axis_1[0].axis('off')\n",
        "\n",
        "# Exibir as linhas encontradas\n",
        "axis_1[1].imshow(cv2.cvtColor(imagem_linhas_1, cv2.COLOR_BGR2RGB))\n",
        "axis_1[1].set_title(\"Linhas de Colagem destacadas 1 (Filtro Passa-Alta)\")\n",
        "axis_1[1].axis('off')\n",
        "\n",
        "# Exibir a imagem após o filtro\n",
        "axis_1[2].imshow(cv2.cvtColor(imagem_resultado_1, cv2.COLOR_BGR2RGB))\n",
        "axis_1[2].set_title(\"Imagem após Filtro Mascarado 1\")\n",
        "axis_1[2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Passo 6: Encontrar bordas novamente\n",
        "imagem_linhas_2 = encontra_bordas(imagem_resultado_1)\n",
        "# Passo 7: Aplicar filtro mascarado novamente\n",
        "imagem_resultado_2 = aplica_filtro_mascarado(imagem_resultado_1, imagem_linhas_2)\n",
        "# Exibir o resultado\n",
        "\n",
        "fig_2, axis_2 = plt.subplots(1, 3, figsize=(60, 12))\n",
        "# Exibir a imagem original\n",
        "axis_2[0].imshow(cv2.cvtColor(imagem_resultado_1, cv2.COLOR_BGR2RGB))\n",
        "axis_2[0].set_title(\"Imagem após Filtro Mascarado 1\")\n",
        "axis_2[0].axis('off')\n",
        "\n",
        "# Exibir as linhas encontradas\n",
        "axis_2[1].imshow(cv2.cvtColor(imagem_linhas_2, cv2.COLOR_BGR2RGB))\n",
        "axis_2[1].set_title(\"Linhas de Colagem destacadas 2 (Filtro Passa-Alta)\")\n",
        "axis_2[1].axis('off')\n",
        "\n",
        "# Exibir a imagem após o filtro\n",
        "axis_2[2].imshow(cv2.cvtColor(imagem_resultado_2, cv2.COLOR_BGR2RGB))\n",
        "axis_2[2].set_title(\"Imagem após Filtro Mascarado 2\")\n",
        "axis_2[2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Salvar as imagens \n",
        "output_naive = output_dir + 'exploracao/'\n",
        "os.makedirs(output_naive, exist_ok=True)\n",
        "\n",
        "cv2.imwrite(output_naive + f'panorama_{cenario}_naive.jpg', panorama_naive)\n",
        "cv2.imwrite(output_naive + f'linhas_1_{cenario}_naive.jpg', imagem_linhas_1)\n",
        "cv2.imwrite(output_naive + f'resultado_1_{cenario}_naive.jpg', imagem_resultado_1)\n",
        "cv2.imwrite(output_naive + f'linhas_2_{cenario}_naive.jpg', imagem_linhas_2)\n",
        "cv2.imwrite(output_naive + f'resultado_2_{cenario}_naive.jpg', imagem_resultado_2)\n",
        "fig_1.savefig(output_naive + f'comparacao_1_{cenario}_naive.jpg')\n",
        "fig_2.savefig(output_naive + f'comparacao_2_{cenario}_naive.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correção de costuras por operações morfológicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"--- Iniciando a criação do panorama morfologia com correção por bordas ---\")\n",
        "\n",
        "# Passo 1: Calcular homografias entre pares de imagens\n",
        "homografias_parciais_morfologia = calcular_todas_as_homografias(images, sift)\n",
        "\n",
        "# Passo 2: Calcular homografias em relação à imagem central\n",
        "idx_ref_morfologia = len(images) // 2\n",
        "homografias_para_referencia = calcular_homografias_para_referencia(homografias_parciais_morfologia, idx_ref_morfologia)\n",
        "\n",
        "# Passo 3: Renderizar o panorama\n",
        "panorama_morfologia = renderizar_panorama_morfologia(images, homografias_para_referencia)\n",
        "\n",
        "# Exibir o panorama gerado\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.imshow(cv2.cvtColor(panorama_morfologia, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Panorama gerado com correções morfológicas e de histograma\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Salvar as imagens \n",
        "output_morfologia = output_dir + 'exploracao/'\n",
        "os.makedirs(output_morfologia, exist_ok=True)\n",
        "\n",
        "cv2.imwrite(output_morfologia + f'panorama_{cenario}_morfologia.jpg', panorama_morfologia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z42upRhctxyF"
      },
      "source": [
        "# Código para excluir as imagens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dc3da28"
      },
      "source": [
        "**CUIDADO:** Este código excluirá permanentemente todos os arquivos com as extensões `.jpeg`, `.jpg`, e `.png` do diretório atual. Certifique-se de que você realmente quer excluir esses arquivos antes de executar a célula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db980dad",
        "outputId": "52a10c2f-845f-4e34-86fa-f350f182e397"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Lista as extensões dos arquivos que você quer excluir\n",
        "extensoes_a_excluir = ['.jpeg', '.jpg', '.png']\n",
        "\n",
        "# Pega a lista de todos os arquivos no diretório atual\n",
        "arquivos_no_diretorio = os.listdir()\n",
        "\n",
        "# Itera sobre os arquivos e exclui aqueles que têm as extensões especificadas\n",
        "arquivos_excluidos = []\n",
        "# for arquivo in arquivos_no_diretorio:\n",
        "#     if any(arquivo.endswith(ext) for ext in extensoes_a_excluir):\n",
        "#         try:\n",
        "#             os.remove(arquivo)\n",
        "#             arquivos_excluidos.append(arquivo)\n",
        "#         except OSError as e:\n",
        "#             print(f\"Erro ao excluir o arquivo {arquivo}: {e}\")\n",
        "\n",
        "print(f\"Arquivos excluídos: {arquivos_excluidos}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mc949",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
